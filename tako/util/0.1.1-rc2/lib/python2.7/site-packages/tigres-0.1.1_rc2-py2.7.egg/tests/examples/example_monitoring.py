"""
Example of use of the Tigres monitoring API.

This is designed to be run as a command-line program, therefore
the standard layout for Python CLI is observed, namely that
argument parsing occurs in the `main` function and this calls some
other function(s), in this case the `run` function, to do the real
work of the program.

Therefore, to see how this program works start reading the `run`
function. This will reference a number of utility functions, etc.
that are defined between the `run` and `main` functions.

"""
from tigres.core.monitoring import log

__author__ = 'Dan Gunter <dkgunter@lbl.gov>'
__date__ = '5/7/13'

## Imports

# Standard library
import random
import sys

import argparse

# Tigres templates
import tigres
from tigres import *
# Tigres monitoring API

######################################################################################

def run(num=1, pnum=1, ofile='', name=None):
    """Run a number of iterations of a workflow, creating and querying the
    monitoring at each iteration.

    The workflows are either a diamond workflow (split, parallel, merge)
    or a pipeline (sequence). At each iteration, the template will be chosen randomly.
    Also log some other "user" information at each iteration.

    :param num: Number of iterations
    :param pnum: Number of parallel/sequential things to run in each iteration
    :param ofile: Log file name
    :return: Status code 0=OK, other=error
    :rtype: int
    """

    templates = {'split': split_workflow, 'parallel': parallel_workflow,
                 'merge': merge_workflow, 'pipeline': pipeline_workflow}

    # Initialize the monitoring API, with output
    # to the file given in `ofile`.
    tigres.start(name=name, log_dest=ofile)


    #
    # Main loop
    #

    for i in range(num):

        #
        # Run workflows
        #

        # Pick a random number to snippets in logs
        val = random.random()
        # randomly choose a type of workflow
        wf_type = random.choice(list(templates.keys()))
        # Write a user-defined log message
        log.write(log.Level.INFO, "random_sample", value=val, wf=wf_type, i=i)
        # Choose the function that implements that type of workflow
        wf_func = templates[wf_type]
        # Invoke and run the workflow
        name, total = wf_func(i, pnum, val)

        # Check correctness
        expect_total = val * pnum
        delta = total - expect_total
        assert -1e-6 < delta < 1e-6, "Wrong total: off by {:f}".format(delta)

        #
        # Query the monitoring information
        #

        # Get the user log entry, using the name we gave earlier
        # and filtering just the current iteration.
        # .. TODO:: epsilon for value??
        records = log.find("random_sample", i=i, value=val)
        # Print what we got for the user log entry
        if len(records) == 0:
            print(("ERROR: No info i={:d}".format(i)))
        else:
            rec = records[0]
            print(("\n[{}] Workflow type: {} (value = {})".format(
                rec['i'], rec['wf'], rec['value'])))
            # Sample output:
            #     (0) Workflow type: pipeline

        # Get the (automatically recorded) status of the template,
        # based on its name
        status = log.check(log.NodeType.TEMPLATE, name)
        # Print what we got for status
        if status is None:
            print("No state")
        else:
            print(("Status: {}".format(status[0])))
            # Sample output:
            #     Status: {"event": "tmpl.sequence.DONE", "state": "DONE", "name": "pipeline-0", "level": 40, "timestr": "2013-05-08T12:03:27.702710", "ts": 1368014607.70271, "errmsg": null, "errcode": 0}

        # Find logs based on value
        # Look for values +/- epsilon to account for differences in
        # floating point representation.
        eps = 1e-6
        queries = ['value > {:.6f}'.format(val - eps),
                   'value < {:.6f}'.format(val + eps)]
        # Loop over all found records, and  project just onto two fields
        # (in addition to default timestamp and level).
        for rec in log.query(queries, ['i', 'wf', 'value']):
            # Print what we got
            print(("Value match: {}".format(rec.to_json())))
            # Sample output:
            #     Value match: {'i': '0', 'wf': 'pipeline', 'ts': 1368055455.64255, 'level': 40}

    tigres.end()
    return 0

######################################################################################
# Workflow functions


def split_workflow(i, n, value):
    """Create and run a split workflow with `n` tasks.

                split
               / | ... \
             /   |      \
        task1  task2 ..  taskn

    :param i: Iteration number.
    :param n: Number of tasks in workflow.
    :return: The name of the template that was run and the result of the run
    :rtype: tuple (name, result)
    """
    name = "split-{:d}".format(i)
    tarr = create_taskarray(n, dummy_task_parallel)
    splitter = Task(None, FUNCTION, split_task)
    splitter_in = InputValues(None, [n, value])
    results = split(name, splitter, splitter_in, tarr)
    result_sum = sum(results)
    return name, result_sum


def merge_workflow(i, n, value):
    """Create and run a merge workflow with `n` tasks.

        task1  task2 ..  taskn
            \    |      /
             \   |    /
              merge

    :param i: Iteration number.
    :param n: Number of tasks in workflow.
    :return: The name of the template that was run and the result of the run
    :rtype: tuple (name, result)
    """
    name = "merge-{:d}".format(i)
    tarr = create_taskarray(n, dummy_task_parallel)
    iarr = create_inputarray([[value]] * n)
    merger = Task(None, FUNCTION, merge_task)
    result_sum = merge(name, tarr, iarr, merger)
    return name, result_sum


def parallel_workflow(i, n, value):
    """Create and run a "diamond" workflow with `n` tasks.

                --------
               / | ... \
              /  |      \
        task1  task2 ..  taskn

    :param i: Iteration number.
    :param n: Number of tasks in workflow.
    :return: The name of the template that was run and the result of the run
    :rtype: tuple (name, result)
    """
    name = "diamond-{:d}".format(i)
    tarr = create_taskarray(n, dummy_task_parallel)
    iarr = create_inputarray([[value]] * n)
    results = parallel(name, tarr, iarr)
    result_sum = sum(results)
    return name, result_sum


def pipeline_workflow(i, n, value):
    """Create and run a "pipeline" workflow with `n` tasks.

             (start)
                |
              task1
                |
              task2
                |
                ...
                |
             taskn
                |
            (finish)

    :param i: Iteration number.
    :param n: Number of tasks in workflow.
    :return: The name of the template that was run and the result of the run
    :rtype: tuple (name, result)
    """
    name = "pipeline-{:d}".format(i)
    tarr = create_taskarray(n, dummy_task_seq)
    iarr = create_inputarray([[0, value]] + [[PREVIOUS, value]] * (n - 1))
    result = sequence(name, tarr, iarr)
    return name, result

######################################################################################
# Task definitions


def dummy_task_parallel(number):
    log.write(log.Level.INFO, "hello_dummy", number=number)
    return number


def dummy_task_seq(prev, number):
    log.write(log.Level.INFO, "hello_dummy", number=number)
    return prev + number


def split_task(n, value):
    return value


def merge_task(values):
    return sum(values)

######################################################################################
# Helper functions


def create_taskarray(n, fn):
    """Wrapper for Tigres API functions to create and return
    a TaskArray of `n` tasks all calling function `fn`.
    """
    tasks = [Task("dummy", FUNCTION, fn) for i in range(n)]
    return TaskArray(None, tasks)


def create_inputarray(values):
    """Wrapper for Tigres API functions to create and return
    an InputArray of `n` inputs each with the same `value`.
    """
    ival = InputValues(None, values)
    return InputArray(None, ival)

######################################################################################
# Command-line functions


def main(cmdline):
    """Program entry point.
    """
    p = argparse.ArgumentParser(description=__doc__)
    p.add_argument('ofile', help='Output log file')
    p.add_argument('-p', dest='pnum', type=int, default=10,
                   help='Number of parallel clients (default=10)')
    p.add_argument('-n', dest='num', type=int, default=10,
                   help='Number of iterations (default=10)')
    p.add_argument('-w', '--workflow', dest='wfname', metavar='NAME', default=None,
                   help="Workflow name")
    # parse args and stuff in global
    args = p.parse_args(cmdline[1:])
    # run
    print(("Running {:d} iterations with {:d} tasks each".format(args.num, args.pnum)))
    print(("Logs in {}".format(args.ofile)))
    # Sample output:
    #    Running 2 iterations with 2 tasks each
    #    Logs in foofile
    result = run(num=args.num, pnum=args.pnum, ofile=args.ofile, name=args.wfname)
    dot_execution()
    return result


if __name__ == '__main__':
    sys.exit(main(sys.argv))
    pass
